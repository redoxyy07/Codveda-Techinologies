Codveda-Technologies – Data Analysis Internship


This repository contains all tasks completed as part of the **Codveda Technologies Data Analysis Internship**, covering Levels 1, 2, and 3.
The work includes data cleaning, analysis, visualizations, regression, classification, and dashboarding using Python.

Datasets Used
Churn 80 Dataset
Description: Customer churn dataset with 80 rows used for prediction and classification.  

Stock Price Dataset
Description: Daily closing prices of companies (e.g., Amazon).

Level 1 : Basic Data Analytics
Task 1: Data Cleaning & Preprocessing
- Loaded data with Pandas
- Handled missing values
- Converted data types
- Filled null values

Task 2: Exploratory Data Analysis (EDA)
- Descriptive statistics (mean, median, std, etc.)
- Visualized distributions using histograms and boxplots
- Analyzed correlations using heatmaps

Task 3: Visualization
- Bar chart
- Line chart
- Scatter plot
- Exported visualizations as PNGs

Outputs: Box plot for Total Day Minutes by Churn Status.png, Line chart for Customer Service Calls vs Total Day Minutes.png, Scatter Plot for Total International Call Minutes vs Charges.png


Level 2: Intermediate Data Analysis
Task 1: Regression Analysis (Churn 80)
- Dataset: Churn 80
- Performed linear regression using scikit-learn
- Split data into training/testing
- Interpreted coefficients and evaluated R², MSE

Task 2: Time Series Analysis (Stock Prices)
- Dataset: Stock Price prediction
- Visualized daily closing prices
- Applied moving average smoothing
- Decomposed time series into trend, seasonality, residuals using statsmodels

Task 3: Classification (Churn 80)
- Dataset: Churn 80
- Preprocessed data and encoded features
- Applied classification models (e.g., Logistic Regression, Decision Trees)
- Evaluated accuracy, precision, recall, and F1-score

Level 3: Advanced Data Projects

Task 1: Dashboard using Tableau
- Created a Tableau dashboard from Churn 80
- Built visual elements like maps, bar charts, line plots
- Added filters and interactivity for better insights

Output File: Tableau.twb

Task 2 : Predictive Analysis
- Built and evaluated ML models (Logistic Regression, Random Forest, Gradient Boosting)
- Tuned hyperparameters using GridSearchCV
- Visualized model performance using accuracy scores and ROC curves

Task 3: Sentiment Analysis (NLP)
- Dataset: Sentiment
- Preprocessed text (tokenization, stopwords, lemmatization)
- Performed sentiment analysis using TextBlob
- Created word clouds and sentiment distribution charts

Tools & Libraries
- Python 
- pandas, numpy
- matplotlib, seaborn
- scikit-learn, statsmodels
- textblob, nltk
- Jupyter Notebook
- Tableau
